---
title: "SPARK: DecisionTree, K-Means, TF-IDF, Linear Regression"
date: 2017-12-09
categories:
  - Cluster Computing
tags:
  - Python
  - R
  - Big data
  - Classification
  - Clustering
  - Regression
  - Spark
---

---
Apache Spark is a open source cluster computing software used for analyzing big data sets. Spark is awesome because with spark you're able to run python scripts on several different computers. This way you're able to analyze huge datasets alot more quickly.

Spark is written in Scala witch is built in Java so we're dealing with a bit of a technological layer here, but we can still use Python code to run our scripts.

Python, Java, Scala

Spark uses something called RDD which stand for Resilient Distributed Dataset. Which are basically items who are able to run on several computers with crashes. When it crashes it's able to recover, ergo resilient.

RDD = Resilient Distributed Dataset

Spark is used for several components in the data science spectrum.

Components of Spark:
* Spark Streaming
* Spark SQL
* MLLib
* GraphX

In this section we'll focus on MLLib, the machine learning part of Spark.

# Decision trees in SPARK

In this section we're going to build a decision tree in spark. It's quite close to building one in Python. For this problem we're going to use a very easy dataset [PastHires.csv](/data/PastHires.csv)

| Years Experience | Employed? | Previous employers | Level of Education | Top-tier school | Interned | Hired |
|------------------|-----------|--------------------|--------------------|-----------------|----------|-------|
| 10               | Y         | 4                  | BS                 | N               | N        | Y     |
| 0                | N         | 0                  | BS                 | Y               | Y        | Y     |
| 7                | N         | 6                  | BS                 | N               | N        | N     |
| 2                | Y         | 1                  | MS                 | Y               | N        | Y     |
| 20               | N         | 2                  | PhD                | Y               | N        | N     |
| 0                | N         | 0                  | PhD                | Y               | Y        | Y     |
| 5                | Y         | 2                  | MS                 | N               | Y        | Y     |
| 3                | N         | 1                  | BS                 | N               | Y        | Y     |
| 15               | Y         | 5                  | BS                 | N               | N        | Y     |
| 0                | N         | 0                  | BS                 | N               | N        | N     |
| 1                | N         | 1                  | PhD                | Y               | N        | N     |
| 4                | Y         | 1                  | BS                 | N               | Y        | Y     |
| 0                | N         | 0                  | PhD                | Y               | N        | Y     |

A dataset containing the past hires of a company, which can be used to help talent searchers find new collegues.

```python
from pyspark.mllib.regression import LabeledPoint
from pyspark.mllib.tree import DecisionTree
from pyspark import SparkConf, SparkContext
from numpy import array

# Boilerplate Spark stuff:
conf = SparkConf().setMaster("local").setAppName("SparkDecisionTree")
sc = SparkContext(conf = conf)

# Some functions that convert our CSV input data into numerical
# features for each job candidate
def binary(YN):
    if (YN == 'Y'):
        return 1
    else:
        return 0

def mapEducation(degree):
    if (degree == 'BS'):
        return 1
    elif (degree =='MS'):
        return 2
    elif (degree == 'PhD'):
        return 3
    else:
        return 0

# Convert a list of raw fields from our CSV file to a
# LabeledPoint that MLLib can use. All data must be numerical...
def createLabeledPoints(fields):
    yearsExperience = int(fields[0])
    employed = binary(fields[1])
    previousEmployers = int(fields[2])
    educationLevel = mapEducation(fields[3])
    topTier = binary(fields[4])
    interned = binary(fields[5])
    hired = binary(fields[6])

    return LabeledPoint(hired, array([yearsExperience, employed,
        previousEmployers, educationLevel, topTier, interned]))

#Load up our CSV file, and filter out the header line with the column names
rawData = sc.textFile("c:/users/1asch/udemy-datascience/PastHires.csv")
header = rawData.first()
rawData = rawData.filter(lambda x:x != header)

# Split each line into a list based on the comma delimiters
csvData = rawData.map(lambda x: x.split(","))

# Convert these lists to LabeledPoints
trainingData = csvData.map(createLabeledPoints)

# Create a test candidate, with 10 years of experience, currently employed,
# 3 previous employers, a BS degree, but from a non-top-tier school where
# he or she did not do an internship. You could of course load up a whole
# huge RDD of test candidates from disk, too.
testCandidates = [ array([10, 1, 3, 1, 0, 0])]
testData = sc.parallelize(testCandidates)

# Train our DecisionTree classifier using our data set
model = DecisionTree.trainClassifier(trainingData, numClasses=2,
                                     categoricalFeaturesInfo={1:2, 3:4, 4:2, 5:2},
                                     impurity='gini', maxDepth=5, maxBins=32)

# Now get predictions for our unknown candidates. (Note, you could separate
# the source data into a training set and a test set while tuning
# parameters and measure accuracy as you go!)
predictions = model.predict(testData)
print('Hire prediction:')
results = predictions.collect()
for result in results:
    print(result)

# We can also print out the decision tree itself:
print('Learned classification tree model:')
print(model.toDebugString())

```

Since now we've created our script we have to run it in the command prompt using spark-submit SparkDecisionTree.py:

```
C:\Users\1asch\udemy-datascience
λ spark-submit SparkDecisionTree.py
17/12/12 15:08:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/12/12 15:08:10 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)
Hire prediction:
1.0
Learned classification tree model:
DecisionTreeModel classifier of depth 4 with 9 nodes
  If (feature 1 in {0.0})
   If (feature 5 in {0.0})
    If (feature 0 <= 0.0)
     If (feature 3 in {1.0})
      Predict: 0.0
     Else (feature 3 not in {1.0})
      Predict: 1.0
    Else (feature 0 > 0.0)
     Predict: 0.0
   Else (feature 5 not in {0.0})
    Predict: 1.0
  Else (feature 1 not in {0.0})
   Predict: 1.0
```

Our decision tree is created and we can conclude that our model hires the test subject:

$$testCandidates = [ array([10, 1, 3, 1, 0, 0])]$$.

Let look at a different model!

# K-means clustering in SPARK

``` python
from pyspark.mllib.clustering import KMeans
from numpy import array, random
from math import sqrt
from pyspark import SparkConf, SparkContext
from sklearn.preprocessing import scale

K = 5

# Boilerplate Spark stuff:
conf = SparkConf().setMaster("local").setAppName("SparkKMeans")
sc = SparkContext(conf = conf)

#Create fake income/age clusters for N people in k clusters
def createClusteredData(N, k):
    random.seed(10)
    pointsPerCluster = float(N)/k
    X = []
    for i in range (k):
        incomeCentroid = random.uniform(20000.0, 200000.0)
        ageCentroid = random.uniform(20.0, 70.0)
        for j in range(int(pointsPerCluster)):
            X.append([random.normal(incomeCentroid, 10000.0), random.normal(ageCentroid, 2.0)])
    X = array(X)
    return X

# Load the data; note I am normalizing it with scale() - very important!
data = sc.parallelize(scale(createClusteredData(100, K)))

# Build the model (cluster the data)
clusters = KMeans.train(data, K, maxIterations=10,
        runs=10, initializationMode="random")

# Print out the cluster assignments
resultRDD = data.map(lambda point: clusters.predict(point)).cache()

print("Counts by value:")
counts = resultRDD.countByValue()
print(counts)

print("Cluster assignments:")
results = resultRDD.collect()
print(results)


# Evaluate clustering by computing Within Set Sum of Squared Errors
def error(point):
    center = clusters.centers[clusters.predict(point)]
    return sqrt(sum([x**2 for x in (point - center)]))

WSSSE = data.map(lambda point: error(point)).reduce(lambda x, y: x + y)
print("Within Set Sum of Squared Error = " + str(WSSSE))

# Things to try:
# What happens to WSSSE as you increase or decrease K? Why?
# What happens if you don't normalize the input data before clustering?
# What happens if you change the maxIterations or runs parameters?
```

Now that we've created our Spark/Python script we can run it in the command prompt by spark-submit SparkKMeans.py! Which yields:

```
C:\Users\1asch\udemy-datascience                                                                     
λ spark-submit SparkKMeans.py                                                                        
17/12/12 15:28:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... us
ing builtin-java classes where applicable                                                            
C:\spark\python\lib\pyspark.zip\pyspark\mllib\clustering.py:347: UserWarning: The param `runs` has no
 effect since Spark 2.0.0.                                                                           
17/12/12 15:28:15 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSyste
mBLAS                                                                                                
17/12/12 15:28:15 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBL
AS                                                                                                   
Counts by value:                                                                                     
defaultdict(<class 'int'>, {2: 20, 4: 20, 3: 23, 0: 17, 1: 20})                                      
Cluster assignments:                                                                                 
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4
, 4, 4, 4, 4, 4, 4, 3, 3, 3, 0, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 0, 3, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 3]   
Within Set Sum of Squared Error = 20.806845084894285                                                 
```

# TF-IDF

TF-IDF stand for Term Frequency and Inverse Document Frequency. This gives important data for search engines. It figures out what terms are most relevant for a document!

**Term Frequency** just measures how often a word occurs in a document. A word that occurs frequently is probably important to that documents meaning.

**Document Frequency** is how often a word occurs in a entire set of documents. This tells us about common words that just appear everywhere no matter what the topic.

So a measure of the relevancy of a word to a document might be:

$$\frac{Term Frequency}{Document Frequency}$$

That is, take how often the word appears in a document, over how often it just appears everywhere. That gives you a measure of how important and unique this word is for this document.

In practice we actually use the log of the IDF, since word frequencies are distributed exponentially. That gives us a better weigthing of a words overall popularity.

Some additional thoughts:
* TF-IDF assumes a document is just a "bag of words".
* Parsing documents into a bag of words can be most of the work.
* Words can be represented as a hash value (number) for efficieny.
* What about synonyms? Various tenses? Abbrevations? Captilizations? Misspellings?
* Doing this at scale is the hard part, that's where SPARK comes in!

So how can me make a search algorithm out of this?
* Compute TF-IDF for every word in a corpus.
* For a given search word, sort the documents by their TF-IDF score for that word.
* Display the results.

Our SPARK script:

``` python
from pyspark import SparkConf, SparkContext
from pyspark.mllib.feature import HashingTF
from pyspark.mllib.feature import IDF

# Boilerplate Spark stuff:
conf = SparkConf().setMaster("local").setAppName("SparkTFIDF")
sc = SparkContext(conf = conf)

# Load documents (one per line).
rawData = sc.textFile("c:/users/1asch/udemy-datascience/subset-small.tsv")
fields = rawData.map(lambda x: x.split("\t"))
documents = fields.map(lambda x: x[3].split(" "))

# Store the document names for later:
documentNames = fields.map(lambda x: x[1])

# Now hash the words in each document to their term frequencies:
hashingTF = HashingTF(100000)  #100K hash buckets just to save some memory
tf = hashingTF.transform(documents)

# At this point we have an RDD of sparse vectors representing each document,
# where each value maps to the term frequency of each unique hash value.

# Let's compute the TF*IDF of each term in each document:
tf.cache()
idf = IDF(minDocFreq=2).fit(tf)
tfidf = idf.transform(tf)

# Now we have an RDD of sparse vectors, where each value is the TFxIDF
# of each unique hash value for each document.

# I happen to know that the article for "Abraham Lincoln" is in our data
# set, so let's search for "Gettysburg" (Lincoln gave a famous speech there):

# First, let's figure out what hash value "Gettysburg" maps to by finding the
# index a sparse vector from HashingTF gives us back:
gettysburgTF = hashingTF.transform(["Gettysburg"])
gettysburgHashValue = int(gettysburgTF.indices[0])

# Now we will extract the TF*IDF score for Gettsyburg's hash value into
# a new RDD for each document:
gettysburgRelevance = tfidf.map(lambda x: x[gettysburgHashValue])

# We'll zip in the document names so we can see which is which:
zippedResults = gettysburgRelevance.zip(documentNames)

# And, print the document with the maximum TF*IDF value:
print("Best document for Gettysburg is:")
print(zippedResults.max())
```

Gives us:

```
C:\Users\1asch\udemy-datascience
λ spark-submit TF-IDF.py
17/12/12 15:56:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Best document for Gettysburg is:
(33.134762509171978, 'Abraham Lincoln')
```

# SPARK 2.0

Now since the newest release of Spark, SPARK 2.0, some changes occured. The major updates are API usability, SQL 2003 support, performance improvements, structured streaming, R UDF support, as well as operational improvements.

One of the largest changes in Spark 2.0 is the new updated APIs:

* Unifying DataFrame and Dataset: In Scala and Java, DataFrame and Dataset have been unified, i.e. DataFrame is just a type alias for Dataset of Row. In Python and R, given the lack of type safety, DataFrame is the main programming interface.
* SparkSession: new entry point that replaces the old SQLContext and HiveContext for DataFrame and Dataset APIs. SQLContext and HiveContext are kept for backward compatibility.
* A new, streamlined configuration API for SparkSession
* Simpler, more performant accumulator API
* A new, improved Aggregator API for typed aggregation in Datasets

Follow along with a Spark/Python script with the new 2.0!

# Linear Regression in SPARK

In the following problem we're using SPARK to predict a person's height according to a person's weight. Beware we normalized the values between -1 and 1. It could be anything really...


``` Python
from __future__ import print_function

from pyspark.ml.regression import LinearRegression

from pyspark.sql import SparkSession
from pyspark.ml.linalg import Vectors

if __name__ == "__main__":

    # Create a SparkSession (Note, the config section is only for Windows!)
    spark = SparkSession.builder.config("spark.sql.warehouse.dir", "file:///C:/temp").appName("LinearRegression").getOrCreate()

    # Load up our data and convert it to the format MLLib expects.
    inputLines = spark.sparkContext.textFile("regression.txt")
    data = inputLines.map(lambda x: x.split(",")).map(lambda x: (float(x[0]), Vectors.dense(float(x[1]))))

    # Convert this RDD to a DataFrame
    colNames = ["label", "features"]
    df = data.toDF(colNames)

    # Note, there are lots of cases where you can avoid going from an RDD to a DataFrame.
    # Perhaps you're importing data from a real database. Or you are using structured streaming
    # to get your data.

    # Let's split our data into training data and testing data
    trainTest = df.randomSplit([0.5, 0.5])
    trainingDF = trainTest[0]
    testDF = trainTest[1]

    # Now create our linear regression model
    lir = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)

    # Train the model using our training data
    model = lir.fit(trainingDF)

    # Now see if we can predict values in our test data.
    # Generate predictions using our linear regression model for all features in our
    # test dataframe:
    fullPredictions = model.transform(testDF).cache()

    # Extract the predictions and the "known" correct labels.
    predictions = fullPredictions.select("prediction").rdd.map(lambda x: x[0])
    labels = fullPredictions.select("label").rdd.map(lambda x: x[0])

    # Zip them together
    predictionAndLabel = predictions.zip(labels).collect()

    # Print out the predicted and actual values for each point
    for prediction in predictionAndLabel:
      print(prediction)


    # Stop the session
    spark.stop()
```


Gives, the predicted values in the first column and the real values in the second column for the test set (50/50)

```
C:\Users\1asch\udemy-datascience
λ spark-submit SparkLinearRegression.py
17/12/12 16:45:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[Stage 2:>                                                          (0 + 2) / 2]17/12/12 16:46:04 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
17/12/12 16:46:04 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS          
17/12/12 16:46:04 WARN BLAS: Fa
(-1.81673833840881, -2.58)     
(-1.6885868850560768, -2.54)   
(-1.660108784311025, -2.29)    
(-1.5461963813308173, -2.27)   
(-1.5461963813308173, -2.17)   
(-1.339730150929191, -2.12)    
(-1.4394035035368726, -2.07)   
(-1.4251644531643468, -2.0)    
(-1.3966863524192947, -1.96)   
(-1.3682082516742429, -1.94)   
(-1.3966863524192947, -1.94)   
(-1.3966863524192947, -1.87)   
(-1.1688615464588796, -1.77)   
(-1.1688615464588796, -1.74)   
...
(-0.1792475455683268, -0.28)   
(-0.21484517149964166, -0.21)  
(-0.18636707075458978, -0.2)   
(-0.2006061211271157, -0.2)    
(-0.07245466777438221, -0.19)  
(-0.1009327685194341, -0.19)   
(-0.051096092215593296, -0.18)
(-0.058215617401856275, -0.18)
(-0.03685704184306736, -0.17)  
(-0.001259415911752498, -0.16)
(-0.15076944482327492, -0.15)  
(-0.14364991963701193, -0.14)  
(-0.1934865959408527, -0.14)   
(-0.015498466284278442, -0.13)
(-0.1009327685194341, -0.13)   
```
